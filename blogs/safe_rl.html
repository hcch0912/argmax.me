<!doctype html>
<html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>HUI CHEN's Site</title>

    <link rel="stylesheet" href="../assets/css/style.css?v=1dfea8a2590028e2ee334e5787d2511685a6ec9e">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="../assets/css/icons.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
    <![endif]-->
</head>

<body>
    <div class="wrapper">


        <header>
            <h3 id="jekyll-themes">Safe Reinforcement Learning Literature Reviews </h3>

            <br>While the accumulative reward is the major optimisation object in reinforcement learning, in some cases, the safety of the agent is also important, like expensive robotics or drones. Safe reinforcement learning is to reduce the risk of
            the algorithm and ensure the safety of the agent. In this article, I will introduce the risks in reinforcment learning and the correpsonding approaches. The reference paper is <i>A Comprehensive Survey of Safe Reinforcement Learning </i>
            <a href="http://www.jmlr.org/papers/volume16/garcia15a/garcia15a.pdf">link</a>

            <br>
            <br>
            <p><a href="../blogs.html">Back to Blog List</a> </p>
        </header>
        <section>
            <h3 id="jekyll-themes"> Risks in Reinforcment Learning</h3>
            <div>
                There are several types of risks existing in reinforcement learning, which come from the stochasticity of the environment. And here I list some of the risks:
                <ul>
                    <li>Variance of the return: even with an optimal policy, the return could be bad in some cases </li>
                    <li> Initial exploration in unknown env could lead to dangerous or dead env </li>
                    <li> Human defined risks, ep: robots going off the road </li>
                </ul>
                From the list we can see that in typical reinforcement learning environment, we usually do not take these conditions into consideration, which makes the agent suffer from risk.
            </div>

            <figure>
                <img src="https://media.giphy.com/media/N8wR1WZobKXaE/giphy.gif" width=200 />
                <figcaption>Image from
                    <a href="https://giphy.com/gifs/robot-competition-department-N8wR1WZobKXaE/links"> gipfy</a>
                </figcaption>
            </figure>
            <br>
            <HR style="FILTER: alpha(opacity=100,finishopacity=0,style=3)" width="100%" color=#987cb9 SIZE=3>

            <h3 id="jekyll-themes"> Approaches to Handle the Risk in Reinforcemnet Learning </h3>
            <div> To handle the risks that listed above, we can add some criterion in the learning so that the agent is able to mitigate the risks. we There are two kinds of criterion, one is Optimisation Criterion, the other is Exploration Process.
                <br>
                <h4>Optimisation Criterion </h4>
                In this category, there are several different criterions. During the training, we need to encode the criterions in the learning system, either in environment or in agent. This will help the reinforcement learning agent learn a safe policy, and a safe
                policy tends to avoid risky actions.
                <br>
                <br>
                <b>1. Worst case criterion:</b>
                <br> We need to find the optimal policy is the one that maximise return in the worst cases. Because even a policy gives good actions in most of the cases, it could kill itself by doing some dangerous action in a worst case. So besides
                a general good policy, we need a safe policy which perform well in extreme conditions.
                <br>
                <br>
                <b>2. Risk sensitive criterion</b>:
                <br>A combination of return and risk, where risk can be defined as the variance of the return. Use this conbinated reward will give the policy a sense of dangerous. This method is easy and straightforward, we just need to make a sensible
                reward function. And there are some examples:
                <br> The optimization objective could be an exponential function of reward and risk :
                <!-- <br> $$ max_{\pi in \Pi} E_{\pi}(R) + \frac{\beta}{2} Var(R) + \BigO(\beta^2) $$, -->
                <br>
                <figure>
                    <img src="images/exp_func.png" height=40/>
                </figure>
                <br> where risk is represented by the variance of reward.
                <br> Also, optimization objective could be the weighted sum of return of risk
                <br>
                <br>
                <b>3. Constrained criterion</b>
                <br>Maximize return while keeping other types of expected measures higher than some given bounds. For example, keep the return exceed some specific mimimum threshold. This is suitable for the case that we don't know what is a good policy
                and we want to do exploration. And another example is to set a threshold of the variance of return. Apart from the reward, we can also set constraints to the policy gradient, like <b>TRPO/ PPO</b>, we cannot update the policy to a directly
                too agressively.
                <br>
                <br>
                <h4> Exploration Process</h4>

                Beside setting criterion, we can also approach the problem from the aspective of exploration.
                <br>
                <br>
                <b> 1. Provide external knowledge </b>
                <br> The criterions could give a the agent a learning signal of what are safe and what are not, where the agents still need to do explorations and learn how to behave. While the other type of method is to teach the agent directly what
                to do and what not to do, which can be more efficient and effective. But this method requires more human effort compare to setting the criterions only.
                <br> In this method, we need someone to provide the initial knowledge, the one could be a human teacher, or an agent with previoud knowledge which is trasferable. The teacher will give exact instructions of what to do and the agent will
                not explore the unknow part of the environment. This method is suitable for the robotic taks, where robos are expensive and fragile and in the meanwhile, there's no much requirement for innovative actions.
                <br>
                <br>
                <b>2.  Risk directed exploration </b>

                <br>Agents are encouraged to seek controllable regions of the environment, and will be punished for exploration directions towards the dangerous places.

            </div>

            <br>
            <div>
                <h3 id="jekyll-themes">End Words</h3>
                Safe reinforcement learning is an really interesting field of research, and it's meaningful for the real-world problems. From robotics to operations, we do not only need a well-performed agent that could out-perform human, but also the agent that is able
                to handle extreme and dangerous cases so that we do not suffer from lose. Anyone with an interest of doing research or practical projects on Safe Reinforcement Learning is welcome to contact me and have discussions.
            </div>
        </section>
    </div>
</body>

</html