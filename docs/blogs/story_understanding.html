<!doctype html>
<html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>HUI CHEN's Site</title>

    <link rel="stylesheet" href="../assets/css/style.css?v=1dfea8a2590028e2ee334e5787d2511685a6ec9e">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="../assets/css/icons.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
</head>

<body>
    <div class="wrapper">


        <header>
            <h3 id="jekyll-themes"> NLP: Story Understanding </h3>
            In the last project, I applied deep learning methods to solve a particular story understanding problem. Automatic understanding of stories is an important task in natural language understanding
            <a href="https://www.aclweb.org/anthology/D13-1020">[1]</a>. Specifically, I developed a model that given a sequence of sentences learns to sort these sentence in order to yield a coherent story <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/short-commonsense-stories.pdf">[2]</a>.
            This sounds (and to an extent is) trivial for humans, however it is quite a difficult task for machines as it involves commonsense knowledge and temporal understanding.

            <p>This is the course project of Statistical Natural Language Processing that I did at UCL in 2017 </p>

            <p><a href="../blogs.html">Back to Blog List</a> </p>
        </header>
        <section>
            <h3 id="jekyll-themes"> Goal</h3>
            The dataset consists of 45502 instances, each consisting of 5 sentences. The system needs to ouput a sequence of numbers which represent the predicted order of these sentences. For example, given a story:
            <div>
                <ul>
                    <li>He went to the store. </li>
                    <li>He found a lamp he liked.</li>
                    <li>He bought the lamp. </li>
                    <li>Jan decided to get a new lamp. </li>
                    <li>Jan's lamp broke. </li>
                </ul>
            </div>


            The system needs to provide an answer in the following form: 2 3 4 1 0 where the numbers correspond to the zero-based index of each sentence in the correctly ordered story. So "2" for "He went to the store." means that this sentence should come 3rd in
            the correctly ordered target story. In this particular example, this order of indices corresponds to the following target story:
            <ol type="1">
                <li>Jan's lamp broke.</li>
                <li>Jan decided to get a new lamp.</li>
                <li>He went to the store.</li>
                <li>He found a lamp he liked.</li>
                <li>He bought the lamp.
                </li>
            </ol>


            <h3 id="jekyll-themes"> My Method</h3>
            <p>
                <h4>Data Preprocessing </h4>
                <div>
                    Tokenization: Used TreebankWordTokenizer from <a href="https://www.nltk.org/_modules/nltk/tokenize/treebank.html">nltk</a> The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank
                </div>
            </p>
            <p>
                <div>Word to Vector: <a href="https://nlp.stanford.edu/projects/glove/">Glove</a> vector representations for words.</div>
            </p>
            <!-- <p>
                <div>Batch and Bucketing: </div>
                <code>

                </code>
            </p> -->
            <p>
                <h4>Model </h4>
                <div>
                    <figure><pre><code>
class BasicRNN():
def __init__(self,num_hidden, num_features, timesteps ):
    self.num_hidden = num_hidden 
    self.num_features = num_features
    self.timesteps = timesteps
                        
    self.weights = {
        'out': tf.Variable(tf.random_normal([self.num_hidden, self.num_features]))
    }
    self.biases = {
        'out': tf.Variable(tf.random_normal([self.num_features]))
    }
                        
def lstm_cell(self):
    return rnn.BasicLSTMCell(self.num_hidden, forget_bias=1.0)
                        
def buildRNN(self, x):
    # Forward direction cell
    rnn_cell = rnn.MultiRNNCell([self.lstm_cell() for _ in range(2)])
    #x = tf.unstack(x, self.timesteps, 1) 
    # x = timesteps x batch_size x input_size
    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)
                        
    return tf.matmul(outputs[-1], self.weights['out']) + self.biases['out']
                        
                        
### MODEL ### 
## PLACEHOLDERS
story = tf.placeholder(tf.int64, [None, None, None], "story")        # [batch_size x 5 x max_length] 
order = tf.placeholder(tf.int64, [None, None], "order")              # [batch_size x 5]
                            
batch_size = tf.shape(story)[0]
                            
sentences = [tf.reshape(x, [batch_size, -1]) for x in tf.split(axis=1, num_or_size_splits=5, value=story)]  # 5 times [batch_size x max_length]
                            
# Word embeddings
nitializer = tf.random_uniform_initializer(-0.1, 0.1)
embeddings = tf.get_variable("W", [vocab_size, input_size], initializer=initializer)
                            
sentences_embedded = [tf.nn.embedding_lookup(embeddings, sentence)  for sentence in sentences]  # [batch_size x max_seq_length x input_size]

basicRNN = BasicRNN(num_hidden, num_features, timesteps )


# # timestep x [batch_size*5, input_size]
# x =  tf.unstack(to_single_sentence, timesteps, 1)

# lstmOutput  [batch_size*5, output_size]
lstmOutput = basicRNN.buildRNN( x )

a = tf.reshape(lstmOutput, [5,-1,64])
sentence_to_vec = tf.unstack(a, 5, 0)

# 5 x [batch x output_size]
sent_lstmOutput = basicRNN.buildBidirectionalRNN( sentence_to_vec )

hs_reshape = tf.concat(sent_lstmOutput, axis = 1)

logits_flat = tf.contrib.layers.linear(hs_reshape, 5 * target_size)    # [batch_size x 5*target_size]
logits_flat_dropout = tf.contrib.layers.dropout(hs_reshape, dropout)
logits = tf.reshape(logits_flat, [-1, 5, target_size])        # [batch_size x 5 x target_size]

# loss + regularization
regularizer = tf.nn.l2_loss(embeddings)
loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=order))
loss = tf.reduce_mean(loss + beta * regularizer)
                  
# loss 
loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=order))
                            
# prediction function
unpacked_logits = [tensor for tensor in tf.unstack(logits, axis=1)]
softmaxes = [tf.nn.softmax(tensor) for tensor in unpacked_logits]
softmaxed_logits = tf.stack(softmaxes, axis=1)
predict = tf.arg_max(softmaxed_logits, 2)
                    </code></pre></figure>
                </div>
            </p>

            <h3 id="jekyll-themes"> Results</h3>

        </section>



        <footer>

            <p>This project is maintained by <a href="http://github.com/hcch0912">hcch0912</a></p>

            <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
        </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>



</body>

</html>