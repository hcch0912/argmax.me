---
layout: article
title: NLP - Story Understanding With LSTM 
tags: NLP 
---


 In the last project, I applied deep learning methods to solve a particular story understanding problem. Automatic understanding of stories is an important task in natural language understanding
            <a href="https://www.aclweb.org/anthology/D13-1020">[1]</a>. Specifically, I developed a model that given a sequence of sentences learns to sort these sentence in order to yield a coherent story <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/short-commonsense-stories.pdf">[2]</a>.
            This sounds (and to an extent is) trivial for humans, however it is quite a difficult task for machines as it involves commonsense knowledge and temporal understanding.

This is the course project of <a href="https://nbviewer.jupyter.org/github/uclmr/stat-nlp-book/blob/python/overview.ipynb" > Statistical Natural Language Processing  </a> that I did at UCL in 2017

### Goal

 The dataset consists of 45502 instances, each consisting of 5 sentences. The system needs to ouput a sequence of numbers which represent the predicted order of these sentences. For example, given a story:

- He went to the store.
- He found a lamp he liked. 
- He bought the lamp. 
- Jan decided to get a new lamp.
- Jan's lamp broke. 

The system needs to provide an answer in the following form: 2 3 4 1 0 where the numbers correspond to the zero-based index of each sentence in the correctly ordered story. So "2" for "He went to the store." means that this sentence should come 3rd in the correctly ordered target story. In this particular example, this order of indices corresponds to the following target story:

<ol type="1">
  <li>Jan's lamp broke.</li>
  <li>Jan decided to get a new lamp.</li>
  <li>He went to the store.</li>
  <li>He found a lamp he liked.</li>
  <li>He bought the lamp.
  </li>
</ol>

### My Method

#### Data Preprocessing 

1. Tokenization: Used TreebankWordTokenizer from <a href="https://www.nltk.org/_modules/nltk/tokenize/treebank.html">nltk</a> The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank

2. Word to Vector: <a href="https://nlp.stanford.edu/projects/glove/">Glove</a> vector representations for words.


#### Model 

```
  from tensorflow.contrib import rnn
  learning_rate = 0.01
  num_hidden = 128
  num_features = 64
  timesteps = max_sent_len
  lstm_output_size = 64

  class BasicRNN():
      def __init__(self,num_hidden, num_features, timesteps ):
          self.num_hidden = num_hidden 
          self.num_features = num_features
          self.timesteps = timesteps


          self.weights = {
              'out': tf.Variable(tf.random_normal([self.num_hidden, self.num_features]))
          }
          self.biases = {
              'out': tf.Variable(tf.random_normal([self.num_features]))
          }

      def lstm_cell(self):
            return rnn.BasicLSTMCell(self.num_hidden, forget_bias=1.0)

      def buildRNN(self, x):
          # Forward direction cell

          rnn_cell = rnn.MultiRNNCell([self.lstm_cell() for _ in range(4)])
          #x = tf.unstack(x, self.timesteps, 1) 
          # x = timesteps x batch_size x input_size
          outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)

          return tf.matmul(outputs[-1], self.weights['out']) + self.biases['out']

      def buildBidirectionalRNN (self, x):
          # Forward direction cell
          lstm_fw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)
          # Backward direction cell
          lstm_bw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)
          # Get lstm cell output
          try:
              outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,
                        dtype=tf.float32)
          except Exception: # Old TensorFlow version only returns outputs not states
              outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,
                  dtype=tf.float32)
          outs = []
          for output in outputs:
              outs.append( tf.matmul(output, self.bi_weights['out']) + self.biases['out'])
          return outs
          
      def buildDynaimcRNN(self, x, batch_size):
          
          lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.num_hidden)
          outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32,
      sequence_length=self.seqlen)
  #         outputs = tf.stack(outputs)
  #         outputs = tf.transpose(outputs, [1, 0, 2])
  #         # Hack to build the indexing and retrieve the right output.
  #         batch_size = tf.shape(outputs)[0]
  #         # Start indices for each sample
  #         index = tf.range(0, batch_size) * self.timesteps + (self.seqlen - 1)
  #         # Indexing
  #         outputs = tf.gather(tf.reshape(outputs, [-1, self.num_hidden]), index)
          outputs = tf.reshape(tf.stack(outputs), [-1, lstm_cell.output_size])
  #         num_partitions = 2
  #         res_out = tf.dynamic_partition(outputs, self.partitions, num_partitions)
          # Linear activation, using outputs computed above
          return tf.matmul(outputs, self.weights['out']) + self.biases['out']
                  
          beta = 0.1
```


#### Training
```


  tf.reset_default_graph()
  tf.initialize_all_variables()


  ## PLACEHOLDERS
  story = tf.placeholder(tf.int64, [None, None, None], "story")        # [batch_size x 5 x max_length]
  order = tf.placeholder(tf.int64, [None, None], "order")              # [batch_size x 5]

  batch_size = tf.shape(story)[0]

  sentences = [tf.reshape(x, [batch_size, -1]) for x in tf.split(axis=1, num_or_size_splits=5, value=story)]  # 5 times [batch_size x max_length]

  # Word embeddings
  initializer = tf.constant_initializer(word_vector_matrix)
  embeddings = tf.get_variable("W", [vocab_size, input_size], initializer=initializer)

  sentences_embedded = [tf.nn.embedding_lookup(embeddings, sentence)   # 5 * [batch_size x max_seq_length x input_size]
                        for sentence in sentences]

  basicRNN = BasicRNN(num_hidden, num_features, timesteps )

  # x [batch_size*5, input_size]

  to_single_sentence = tf.concat(sentences_embedded , axis = 0)

  x =  tf.unstack(to_single_sentence, timesteps, 1)


  # lstmOutput  [batch_size*5, output_size]
  lstmOutput = basicRNN.buildRNN( x )

  # separate lstmOutput 5x[batch_size, output_size]
  hs = tf.reshape(lstmOutput,[5, -1, lstm_output_size] )
  hs_transpose = tf.transpose(hs, [1,0,2])

  hs_reshape = tf.reshape(hs_transpose, [-1, 5*lstm_output_size]) #[batch_size, 5*output]

  logits_flat = tf.contrib.layers.linear(hs_reshape, 5 * target_size)    # [batch_size x 5*target_size]
  logits = tf.reshape(logits_flat, [-1, 5, target_size])        # [batch_size x 5 x target_size]

  # loss + regularization
  regularizer = tf.nn.l2_loss(embeddings)
  loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=order))
  loss = tf.reduce_mean(loss + beta * regularizer)

  # prediction function
  unpacked_logits = [tensor for tensor in tf.unstack(logits, axis=1)]
  softmaxes = [tf.nn.softmax(tensor) for tensor in unpacked_logits]
  softmaxed_logits = tf.stack(softmaxes, axis=1)
  predict = tf.arg_max(softmaxed_logits, 2)

```

### Results

 <table>
                <tr>
                    <th> Model </th>
                    <th> Hyper-parameters ></th>

                    <th> Highest Score </th>
                </tr>
                <tr>
                    <td> Static RNN + LSTM(4) </td>
                    <td> batch size: 256; learning rate: 0.1; </td>
                    <td> 56.0 </td>
                </tr>
                <tr>
                    <td> Bi-directional RNN + LSTM(2) </td>
                    <td> batch size: 256; learning rate: 0.01; </td>
                    <td> 54. </td>
                </tr>
                <tr>
                    <td> Dynamic RNN + LSTM(2) </td>
                    <td> batch size: 256; learning rate: 0.01; </td>
                    <td> 54.6 </td>
                </tr>
                <tr>
                    <td> Sent2Vec RNN </td>
                    <td> batch size: 256; learning rate: 0.01; </td>
                    <td> 53.5 </td>
                </tr>
            </table>

#### Benchmarks
<table>
                <tr>
                    <th> Model </th>

                    <th> Highest Score </th>
                </tr>
                <tr>
                    <td> DSSM</td>
                    <td> 58.5 </td>
                </tr>
                <tr>
                    <td> GenSim</td>
                    <td> 53.9 </td>
                </tr>
                <tr>
                    <td> Skip-thoughts</td>
                    <td> 55.2 </td>
                </tr>
                <tr>
                    <td> SentimentLast</td>
                    <td> 55.2 </td>
                </tr>

                <tr>
                    <td> Frequency </td>
                    <td> 52.0 </td>
                </tr>
                <tr>
                    <td> N-gram-overlap</td>
                    <td> 49.4 </td>
                </tr>
                <tr>
                    <td> Narritive-chains-stories </td>
                    <td> 49.4 </td>
                </tr>
            </table>
