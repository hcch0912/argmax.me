---
layout: article
title: Multi-agent Reinforcement Learning Literature Reviews
tags: RL Multi-agent
---



  *While deep reinforcement learning has been proved successful in many domains, solutions like deep Q-learning and Policy Gradient methods [Mnih et al., 2013, Lillicrap et al., 2015, Silver et al., 2014] can achieve impressive results in some challenging
  single agent environment such as Atari games. However, the multi-agent environment makes the learning problem more challenging ,because agents policies are changing during the training and the environment is not stationary for a single agent
  algorithm to learn a good strategy.
  I am interested in this field of research so I would like to share a review of the literatures. I have categorized the literatures into three groups: opponent modeling methods, centralized critic methods and communication methods.*


## Opponent Modeling Methods

*Due to the changing policies of other agents, the environment is not stationary. To handle this problem, we can build a model of others' policies, to predict the possile next actions of others, and take these actions into account when making decisions.
In this way, the non-stationarity can be mitigated. And with deep learning, we can model others' policies with neural networks, which are learnable. In this section, I will introduce some of the work using opponent modelling method to solve multi-agent problems.*

#### <a href="https://arxiv.org/abs/1709.04326">  Learning with Opponent-Learning Awareness</a>

LOLA's value estimination is not only based on the agent's current policy, but also the other's policy after one anticipated update. LOLA agents do not predict the opponent's policy update and learn a best respone, but attempt to influence the other's policy update. They are shaping the update direction of each other. In experiments, the LOLA agents are able to reach a Nash-equilibrium tic-tac-toc.
              
The code is in <a href="github.com/alshedivat/lola">github repo</a>

####  <a href="https://arxiv.org/pdf/1802.09640.pdf">Modeling other agents by oneself </a>

In this method (SOM), each agent holds a belief of other agents, and they will use its own policy to predict the other's action and to update the belief. The main intention of this work is to make the agents learn other's goal so that they can they can understand each other in order to cooperate. They use recurrent neural network with state and a goal as input. The agent needs to infer others' goal and maximize the likelihood of other's actions.

<img class="image image--lg"  src="/images/som.png"/>

Architecture of SOM <a href="ttps://arxiv.org/pdf/1802.09640.pdf"> (image source) </a>


## Centralized Critic Methods

*Besides building a model other others, another method is to use a centralized critic, which gives the value estimation based on the information from all the agents, so that the non-stationarity could be mitigated. I am not saying elimated because the
critic only gives estimated value, so that it could not model the dynamics perfectly. The advantage of this method is that, we can apply the centralized critic directly in the actor-critic architecture without extra neural networks compared to opponent-modelling method, so it's easier to train.*

#### <a href="https://arxiv.org/abs/1706.02275">Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments

MADDPG has been qualified as the state-of-art multi-agent reinforcement learning architecture since it was published.
The main idea in this work is to use a centralized critic to give the

 *action-value* estimination with the actions of all agents, where the value estimination contrains the information of the global state. This solve the problem of partial observability and non-stationarity that brought by the changing policies of other agents in the environment.

The experiment environment is <a href="https://github.com/openai/multiagent-particle-envs"> OpenAI's multiagent-particle-envs </a>, and code is in <a href="https://github.com/openai/maddpg">github </a>

<img class="image image--lg"  src="/images/maddpg.png"/>

Architecture of MADDPG <a href="https://arxiv.org/abs/1706.02275"> (image source) </a>


#### <a href="https://arxiv.org/abs/1705.08926"> Counterfactual Multi-agent reinforcement learning </a> 

Similar to MADDPG, COMA uses a centralized critic to estiminate the A-function. What's different is that COMA added a **counterfactual baseline** to margin out a single agent's action. So that the credit that the agent recieves can be considered as independent of other agents.
They used StarCrat unit micromanagement as the experiment environment.

<img class="image image--lg"  src="/images/coma.png"/>

 Architecture of COMA <a href="https://arxiv.org/abs/1705.08926"> (image source) </a>


#### <a href="https://arxiv.org/pdf/1810.02912.pdf> Actor-Attention-Critic for Multi-Agent Reinforcement Learning </a>

This method adopts the attention mechanism to control the intake the information from other agent. The advantage of using Attention compared to the pure centralised critic MADDPG is that it does not use all observation equally, more efficient. The critic takes the whole map as input for every agent leads to low exploration problem.   So the input to the critic can scale easily. And compare to COMA, coma performs better when there’s global reward. Because COMA use a single Q network for all agents. MAAC is better when there’s different type of agents. Instead of using the base to calculate the advantage, they use the embedding of the values from attention model as part of the input to the q network.. Because the counterfactual reward equals the counterfactual of the actions, which is the embedding from attention model. The advantage of this is that the agents can have different action space and doesn’t need to be fully observable.  Since the method assumes that it's a centralised training and decentralised execution setting, so the attention mechanism only exist for the critic, actually, if we can use it also for the policy, it might have a boost to the performance. 



## Communication Methods

*In nature, if two creatures want to cooperate, they will share some information to coordinate. So in this set of method, instead of doing modeling or estimination of other's actions or intents, the agent send messages directly to each other. The advantage of this method is that without estimation all the actions and intents are correct. But the disadvantage of this method is that in many cases it's difficult or impossible to construct a message channel. And the cost of sending messages
could be expensive.*

#### <a href="https://arxiv.org/pdf/1605.06676.pdf"> Learning to communicate with deep multi-agent reinforcement learning</a>

This work introduces an architecture where agents can communicate via a limited bandit channel. There are two method of communication, one is to decompose the action into environment action and communication action, to learn them separatedly with two neural networks; the other one is to share the gradients among agents via the communication channel.


####  <a href="https://people.cs.umass.edu/~mahadeva/papers/aamas04.pdf">Learning to Communicate and Act in Cooperative Multiagent Systems using Hierarchical Reinforcement Learning</a>

In this paper, the authors proposed a Hierarchical multi-agent learning method, where the agents learn to communicate or not. Compared to other communication method, the novelty of this method is that the they let the agent to find the balance of using communication or not. Because usually the communication could be expensive, if the agent can decide when to use communication only if necessary, the efficiency would be improved.


#### <a href="https://papers.nips.cc/paper/7956-learning-attentional-communication-for-multi-agent-cooperation.pdf"> Learning Attentional Communication for Multi-Agent Cooperation </a>

Similar to the MAAC in previous section, this method also uses Attention mechanism, but for policy learning. The purpose of using Attention is to select agents that one wants to communicate with, so that they don't need to talk to every one in the space. This improves the communication efficency in a very straight forward and simple way. As you can see, some theortical problems can be solved with new neural network architectures easily. So I believe as more and more new NN technology develops, the multi-agent problems will be easier to tackle. 

<!--more-->